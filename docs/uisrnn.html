<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.2" />
<title>uisrnn.uisrnn API documentation</title>
<meta name="description" content="The UIS-RNN model." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}#index .two-column{column-count:2}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>uisrnn.uisrnn</code> module</h1>
</header>
<section id="section-intro">
<p>The UIS-RNN model.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
&#34;&#34;&#34;The UIS-RNN model.&#34;&#34;&#34;

import functools
import numpy as np
import torch
from torch import autograd
from torch import multiprocessing
from torch import nn
from torch import optim
import torch.nn.functional as F

from uisrnn import loss_func
from uisrnn import utils

_INITIAL_SIGMA2_VALUE = 0.1


class CoreRNN(nn.Module):
  &#34;&#34;&#34;The core Recurent Neural Network used by UIS-RNN.&#34;&#34;&#34;

  def __init__(self, input_dim, hidden_size, depth, observation_dim, dropout=0):
    super(CoreRNN, self).__init__()
    self.hidden_size = hidden_size
    if depth &gt;= 2:
      self.gru = nn.GRU(input_dim, hidden_size, depth, dropout=dropout)
    else:
      self.gru = nn.GRU(input_dim, hidden_size, depth)
    self.linear_mean1 = nn.Linear(hidden_size, hidden_size)
    self.linear_mean2 = nn.Linear(hidden_size, observation_dim)

  def forward(self, input_seq, hidden=None):
    output_seq, hidden = self.gru(input_seq, hidden)
    if isinstance(output_seq, torch.nn.utils.rnn.PackedSequence):
      output_seq, _ = torch.nn.utils.rnn.pad_packed_sequence(
          output_seq, batch_first=False)
    mean = self.linear_mean2(F.relu(self.linear_mean1(output_seq)))
    return mean, hidden


class BeamState:
  &#34;&#34;&#34;Structure that contains necessary states for beam search.&#34;&#34;&#34;

  def __init__(self, source=None):
    if not source:
      self.mean_set = []
      self.hidden_set = []
      self.neg_likelihood = 0
      self.trace = []
      self.block_counts = []
    else:
      self.mean_set = source.mean_set.copy()
      self.hidden_set = source.hidden_set.copy()
      self.trace = source.trace.copy()
      self.block_counts = source.block_counts.copy()
      self.neg_likelihood = source.neg_likelihood

  def append(self, mean, hidden, cluster):
    &#34;&#34;&#34;Append new item to the BeamState.&#34;&#34;&#34;
    self.mean_set.append(mean.clone())
    self.hidden_set.append(hidden.clone())
    self.block_counts.append(1)
    self.trace.append(cluster)


class UISRNN:
  &#34;&#34;&#34;Unbounded Interleaved-State Recurrent Neural Networks.&#34;&#34;&#34;

  def __init__(self, args):
    &#34;&#34;&#34;Construct the UISRNN object.

    Args:
      args: Model configurations. See `arguments.py` for details.
    &#34;&#34;&#34;
    self.observation_dim = args.observation_dim
    self.device = torch.device(
        &#39;cuda:0&#39; if (torch.cuda.is_available() and args.enable_cuda) else &#39;cpu&#39;)
    self.rnn_model = CoreRNN(self.observation_dim, args.rnn_hidden_size,
                             args.rnn_depth, self.observation_dim,
                             args.rnn_dropout).to(self.device)
    self.rnn_init_hidden = nn.Parameter(
        torch.zeros(args.rnn_depth, 1, args.rnn_hidden_size).to(self.device))
    # booleans indicating which variables are trainable
    self.estimate_sigma2 = (args.sigma2 is None)
    self.estimate_transition_bias = (args.transition_bias is None)
    # initial values of variables
    sigma2 = _INITIAL_SIGMA2_VALUE if self.estimate_sigma2 else args.sigma2
    self.sigma2 = nn.Parameter(
        sigma2 * torch.ones(self.observation_dim).to(self.device))
    self.transition_bias = args.transition_bias
    self.transition_bias_denominator = 0.0
    self.crp_alpha = args.crp_alpha
    self.logger = utils.Logger(args.verbosity)

  def _get_optimizer(self, optimizer, learning_rate):
    &#34;&#34;&#34;Get optimizer for UISRNN.

    Args:
      optimizer: string - name of the optimizer.
      learning_rate: - learning rate for the entire model.
        We do not customize learning rate for separate parts.

    Returns:
      a pytorch &#34;optim&#34; object
    &#34;&#34;&#34;
    params = [
        {
            &#39;params&#39;: self.rnn_model.parameters()
        },  # rnn parameters
        {
            &#39;params&#39;: self.rnn_init_hidden
        }  # rnn initial hidden state
    ]
    if self.estimate_sigma2:  # train sigma2
      params.append({
          &#39;params&#39;: self.sigma2
      })  # variance parameters
    assert optimizer == &#39;adam&#39;, &#39;Only adam optimizer is supported.&#39;
    return optim.Adam(params, lr=learning_rate)

  def save(self, filepath):
    &#34;&#34;&#34;Save the model to a file.

    Args:
      filepath: the path of the file.
    &#34;&#34;&#34;
    torch.save({
        &#39;rnn_state_dict&#39;: self.rnn_model.state_dict(),
        &#39;rnn_init_hidden&#39;: self.rnn_init_hidden.detach().cpu().numpy(),
        &#39;transition_bias&#39;: self.transition_bias,
        &#39;transition_bias_denominator&#39;: self.transition_bias_denominator,
        &#39;crp_alpha&#39;: self.crp_alpha,
        &#39;sigma2&#39;: self.sigma2.detach().cpu().numpy()}, filepath)

  def load(self, filepath):
    &#34;&#34;&#34;Load the model from a file.

    Args:
      filepath: the path of the file.
    &#34;&#34;&#34;
    var_dict = torch.load(filepath)
    self.rnn_model.load_state_dict(var_dict[&#39;rnn_state_dict&#39;])
    self.rnn_init_hidden = nn.Parameter(
        torch.from_numpy(var_dict[&#39;rnn_init_hidden&#39;]).to(self.device))
    self.transition_bias = float(var_dict[&#39;transition_bias&#39;])
    self.transition_bias_denominator = float(
        var_dict[&#39;transition_bias_denominator&#39;])
    self.crp_alpha = float(var_dict[&#39;crp_alpha&#39;])
    self.sigma2 = nn.Parameter(
        torch.from_numpy(var_dict[&#39;sigma2&#39;]).to(self.device))

    self.logger.print(
        3, &#39;Loaded model with transition_bias={}, crp_alpha={}, sigma2={}, &#39;
        &#39;rnn_init_hidden={}&#39;.format(
            self.transition_bias, self.crp_alpha, var_dict[&#39;sigma2&#39;],
            var_dict[&#39;rnn_init_hidden&#39;]))

  def fit_concatenated(self, train_sequence, train_cluster_id, args):
    &#34;&#34;&#34;Fit UISRNN model to concatenated sequence and cluster_id.

    Args:
      train_sequence: the training observation sequence, which is a
        2-dim numpy array of real numbers, of size `N * D`.

        - `N`: summation of lengths of all utterances.
        - `D`: observation dimension.

        For example,
      ```
      train_sequence =
      [[1.2 3.0 -4.1 6.0]    --&gt; an entry of speaker #0 from utterance &#39;iaaa&#39;
       [0.8 -1.1 0.4 0.5]    --&gt; an entry of speaker #1 from utterance &#39;iaaa&#39;
       [-0.2 1.0 3.8 5.7]    --&gt; an entry of speaker #0 from utterance &#39;iaaa&#39;
       [3.8 -0.1 1.5 2.3]    --&gt; an entry of speaker #0 from utterance &#39;ibbb&#39;
       [1.2 1.4 3.6 -2.7]]   --&gt; an entry of speaker #0 from utterance &#39;ibbb&#39;
      ```
        Here `N=5`, `D=4`.

        We concatenate all training utterances into this single sequence.
      train_cluster_id: the speaker id sequence, which is 1-dim list or
        numpy array of strings, of size `N`.
        For example,
      ```
      train_cluster_id =
        [&#39;iaaa_0&#39;, &#39;iaaa_1&#39;, &#39;iaaa_0&#39;, &#39;ibbb_0&#39;, &#39;ibbb_0&#39;]
      ```
        &#39;iaaa_0&#39; means the entry belongs to speaker #0 in utterance &#39;iaaa&#39;.

        Note that the order of entries within an utterance are preserved,
        and all utterances are simply concatenated together.
      args: Training configurations. See `arguments.py` for details.

    Raises:
      TypeError: If train_sequence or train_cluster_id is of wrong type.
      ValueError: If train_sequence or train_cluster_id has wrong dimension.
    &#34;&#34;&#34;
    # check type
    if (not isinstance(train_sequence, np.ndarray) or
        train_sequence.dtype != float):
      raise TypeError(&#39;train_sequence should be a numpy array of float type.&#39;)
    if isinstance(train_cluster_id, list):
      train_cluster_id = np.array(train_cluster_id)
    if (not isinstance(train_cluster_id, np.ndarray) or
        not train_cluster_id.dtype.name.startswith((&#39;str&#39;, &#39;unicode&#39;))):
      raise TypeError(&#39;train_cluster_id type be a numpy array of strings.&#39;)
    # check dimension
    if train_sequence.ndim != 2:
      raise ValueError(&#39;train_sequence must be 2-dim array.&#39;)
    if train_cluster_id.ndim != 1:
      raise ValueError(&#39;train_cluster_id must be 1-dim array.&#39;)
    # check length and size
    train_total_length, observation_dim = train_sequence.shape
    if observation_dim != self.observation_dim:
      raise ValueError(&#39;train_sequence does not match the dimension specified &#39;
                       &#39;by args.observation_dim.&#39;)
    if train_total_length != len(train_cluster_id):
      raise ValueError(&#39;train_sequence length is not equal to &#39;
                       &#39;train_cluster_id length.&#39;)

    self.rnn_model.train()
    optimizer = self._get_optimizer(optimizer=args.optimizer,
                                    learning_rate=args.learning_rate)

    sub_sequences, seq_lengths = utils.resize_sequence(
        sequence=train_sequence,
        cluster_id=train_cluster_id,
        num_permutations=args.num_permutations)

    # For batch learning, pack the entire dataset.
    if args.batch_size is None:
      packed_train_sequence, rnn_truth = utils.pack_sequence(
          sub_sequences,
          seq_lengths,
          args.batch_size,
          self.observation_dim,
          self.device)
    train_loss = []
    for num_iter in range(args.train_iteration):
      optimizer.zero_grad()
      # For online learning, pack a subset in each iteration.
      if args.batch_size is not None:
        packed_train_sequence, rnn_truth = utils.pack_sequence(
            sub_sequences,
            seq_lengths,
            args.batch_size,
            self.observation_dim,
            self.device)
      hidden = self.rnn_init_hidden.repeat(1, args.batch_size, 1)
      mean, _ = self.rnn_model(packed_train_sequence, hidden)
      # use mean to predict
      mean = torch.cumsum(mean, dim=0)
      mean_size = mean.size()
      mean = torch.mm(
          torch.diag(
              1.0 / torch.arange(1, mean_size[0] + 1).float().to(self.device)),
          mean.view(mean_size[0], -1))
      mean = mean.view(mean_size)

      # Likelihood part.
      loss1 = loss_func.weighted_mse_loss(
          input_tensor=(rnn_truth != 0).float() * mean[:-1, :, :],
          target_tensor=rnn_truth,
          weight=1 / (2 * self.sigma2))

      # Sigma2 prior part.
      weight = (((rnn_truth != 0).float() * mean[:-1, :, :] - rnn_truth)
                ** 2).view(-1, observation_dim)
      num_non_zero = torch.sum((weight != 0).float(), dim=0).squeeze()
      loss2 = loss_func.sigma2_prior_loss(
          num_non_zero, args.sigma_alpha, args.sigma_beta, self.sigma2)

      # Regularization part.
      loss3 = loss_func.regularization_loss(
          self.rnn_model.parameters(), args.regularization_weight)

      loss = loss1 + loss2 + loss3
      loss.backward()
      nn.utils.clip_grad_norm_(self.rnn_model.parameters(), args.grad_max_norm)
      optimizer.step()
      # avoid numerical issues
      self.sigma2.data.clamp_(min=1e-6)

      if (np.remainder(num_iter, 10) == 0 or
          num_iter == args.train_iteration - 1):
        self.logger.print(
            2,
            &#39;Iter: {:d}  \t&#39;
            &#39;Training Loss: {:.4f}    \n&#39;
            &#39;    Negative Log Likelihood: {:.4f}\t&#39;
            &#39;Sigma2 Prior: {:.4f}\t&#39;
            &#39;Regularization: {:.4f}&#39;.format(
                num_iter,
                float(loss.data),
                float(loss1.data),
                float(loss2.data),
                float(loss3.data)))
      train_loss.append(float(loss1.data))  # only save the likelihood part
    self.logger.print(
        1, &#39;Done training with {} iterations&#39;.format(args.train_iteration))

  def fit(self, train_sequences, train_cluster_ids, args):
    &#34;&#34;&#34;Fit UISRNN model.

    Args:
      train_sequences: Either a list of training sequences, or a single
        concatenated training sequence:

        1. train_sequences is list, and each element is a 2-dim numpy array
           of real numbers, of size: `length * D`.
           The length varies among different sequences, but the D is the same.
           In speaker diarization, each sequence is the sequence of speaker
           embeddings of one utterance.
        2. train_sequences is a single concatenated sequence, which is a
           2-dim numpy array of real numbers. See `fit_concatenated()`
           for more details.
      train_cluster_ids: Ground truth labels for train_sequences:

        1. if train_sequences is a list, this must also be a list of the same
           size, each element being a 1-dim list or numpy array of strings.
        2. if train_sequences is a single concatenated sequence, this
           must also be the concatenated 1-dim list or numpy array of strings
      args: Training configurations. See `arguments.py` for details.

    Raises:
      TypeError: If train_sequences or train_cluster_ids is of wrong type.
    &#34;&#34;&#34;
    if isinstance(train_sequences, np.ndarray):
      # train_sequences is already the concatenated sequence
      if self.estimate_transition_bias:
        # see issue #55: https://github.com/google/uis-rnn/issues/55
        self.logger.print(
            2,
            &#39;Warning: transition_bias cannot be correctly estimated from a &#39;
            &#39;concatenated sequence; train_sequences will be treated as a &#39;
            &#39;single sequence. This can lead to inaccurate estimation of &#39;
            &#39;transition_bias. Please, consider estimating transition_bias &#39;
            &#39;before concatenating the sequences and passing it as argument.&#39;)
      train_sequences = [train_sequences]
      train_cluster_ids = [train_cluster_ids]
    elif isinstance(train_sequences, list):
      # train_sequences is a list of un-concatenated sequences
      # we will concatenate it later, after estimating transition_bias
      pass
    else:
      raise TypeError(&#39;train_sequences must be a list or numpy.ndarray&#39;)

    # estimate transition_bias
    if self.estimate_transition_bias:
      (transition_bias,
       transition_bias_denominator) = utils.estimate_transition_bias(
           train_cluster_ids)
      # set or update transition_bias
      if self.transition_bias is None:
        self.transition_bias = transition_bias
        self.transition_bias_denominator = transition_bias_denominator
      else:
        self.transition_bias = (
            self.transition_bias * self.transition_bias_denominator +
            transition_bias * transition_bias_denominator) / (
                self.transition_bias_denominator + transition_bias_denominator)
        self.transition_bias_denominator += transition_bias_denominator

    # concatenate train_sequences
    (concatenated_train_sequence,
     concatenated_train_cluster_id) = utils.concatenate_training_data(
         train_sequences,
         train_cluster_ids,
         args.enforce_cluster_id_uniqueness,
         True)

    self.fit_concatenated(
        concatenated_train_sequence, concatenated_train_cluster_id, args)

  def _update_beam_state(self, beam_state, look_ahead_seq, cluster_seq):
    &#34;&#34;&#34;Update a beam state given a look ahead sequence and known cluster
    assignments.

    Args:
      beam_state: A BeamState object.
      look_ahead_seq: Look ahead sequence, size: look_ahead*D.
        look_ahead: number of step to look ahead in the beam search.
        D: observation dimension
      cluster_seq: Cluster assignment sequence for look_ahead_seq.

    Returns:
      new_beam_state: An updated BeamState object.
    &#34;&#34;&#34;

    loss = 0
    new_beam_state = BeamState(beam_state)
    for sub_idx, cluster in enumerate(cluster_seq):
      if cluster &gt; len(new_beam_state.mean_set):  # invalid trace
        new_beam_state.neg_likelihood = float(&#39;inf&#39;)
        break
      elif cluster &lt; len(new_beam_state.mean_set):  # existing cluster
        last_cluster = new_beam_state.trace[-1]
        loss = loss_func.weighted_mse_loss(
            input_tensor=torch.squeeze(new_beam_state.mean_set[cluster]),
            target_tensor=look_ahead_seq[sub_idx, :],
            weight=1 / (2 * self.sigma2)).cpu().detach().numpy()
        if cluster == last_cluster:
          loss -= np.log(1 - self.transition_bias)
        else:
          loss -= np.log(self.transition_bias) + np.log(
              new_beam_state.block_counts[cluster]) - np.log(
                  sum(new_beam_state.block_counts) + self.crp_alpha)
        # update new mean and new hidden
        mean, hidden = self.rnn_model(
            look_ahead_seq[sub_idx, :].unsqueeze(0).unsqueeze(0),
            new_beam_state.hidden_set[cluster])
        new_beam_state.mean_set[cluster] = (new_beam_state.mean_set[cluster]*(
            (np.array(new_beam_state.trace) == cluster).sum() -
            1).astype(float) + mean.clone()) / (
                np.array(new_beam_state.trace) == cluster).sum().astype(
                    float)  # use mean to predict
        new_beam_state.hidden_set[cluster] = hidden.clone()
        if cluster != last_cluster:
          new_beam_state.block_counts[cluster] += 1
        new_beam_state.trace.append(cluster)
      else:  # new cluster
        init_input = autograd.Variable(
            torch.zeros(self.observation_dim)
        ).unsqueeze(0).unsqueeze(0).to(self.device)
        mean, hidden = self.rnn_model(init_input,
                                      self.rnn_init_hidden)
        loss = loss_func.weighted_mse_loss(
            input_tensor=torch.squeeze(mean),
            target_tensor=look_ahead_seq[sub_idx, :],
            weight=1 / (2 * self.sigma2)).cpu().detach().numpy()
        loss -= np.log(self.transition_bias) + np.log(
            self.crp_alpha) - np.log(
                sum(new_beam_state.block_counts) + self.crp_alpha)
        # update new min and new hidden
        mean, hidden = self.rnn_model(
            look_ahead_seq[sub_idx, :].unsqueeze(0).unsqueeze(0),
            hidden)
        new_beam_state.append(mean, hidden, cluster)
      new_beam_state.neg_likelihood += loss
    return new_beam_state

  def _calculate_score(self, beam_state, look_ahead_seq):
    &#34;&#34;&#34;Calculate negative log likelihoods for all possible state allocations
       of a look ahead sequence, according to the current beam state.

    Args:
      beam_state: A BeamState object.
      look_ahead_seq: Look ahead sequence, size: look_ahead*D.
        look_ahead: number of step to look ahead in the beam search.
        D: observation dimension

    Returns:
      beam_score_set: a set of scores for each possible state allocation.
    &#34;&#34;&#34;

    look_ahead, _ = look_ahead_seq.shape
    beam_num_clusters = len(beam_state.mean_set)
    beam_score_set = float(&#39;inf&#39;) * np.ones(
        beam_num_clusters + 1 + np.arange(look_ahead))
    for cluster_seq, _ in np.ndenumerate(beam_score_set):
      updated_beam_state = self._update_beam_state(beam_state,
                                                   look_ahead_seq, cluster_seq)
      beam_score_set[cluster_seq] = updated_beam_state.neg_likelihood
    return beam_score_set

  def predict_single(self, test_sequence, args):
    &#34;&#34;&#34;Predict labels for a single test sequence using UISRNN model.

    Args:
      test_sequence: the test observation sequence, which is 2-dim numpy array
        of real numbers, of size `N * D`.

        - `N`: length of one test utterance.
        - `D` : observation dimension.

        For example:
      ```
      test_sequence =
      [[2.2 -1.0 3.0 5.6]    --&gt; 1st entry of utterance &#39;iccc&#39;
       [0.5 1.8 -3.2 0.4]    --&gt; 2nd entry of utterance &#39;iccc&#39;
       [-2.2 5.0 1.8 3.7]    --&gt; 3rd entry of utterance &#39;iccc&#39;
       [-3.8 0.1 1.4 3.3]    --&gt; 4th entry of utterance &#39;iccc&#39;
       [0.1 2.7 3.5 -1.7]]   --&gt; 5th entry of utterance &#39;iccc&#39;
      ```
        Here `N=5`, `D=4`.
      args: Inference configurations. See `arguments.py` for details.

    Returns:
      predicted_cluster_id: predicted speaker id sequence, which is
        an array of integers, of size `N`.
        For example, `predicted_cluster_id = [0, 1, 0, 0, 1]`

    Raises:
      TypeError: If test_sequence is of wrong type.
      ValueError: If test_sequence has wrong dimension.
    &#34;&#34;&#34;
    # check type
    if (not isinstance(test_sequence, np.ndarray) or
        test_sequence.dtype != float):
      raise TypeError(&#39;test_sequence should be a numpy array of float type.&#39;)
    # check dimension
    if test_sequence.ndim != 2:
      raise ValueError(&#39;test_sequence must be 2-dim array.&#39;)
    # check size
    test_sequence_length, observation_dim = test_sequence.shape
    if observation_dim != self.observation_dim:
      raise ValueError(&#39;test_sequence does not match the dimension specified &#39;
                       &#39;by args.observation_dim.&#39;)

    self.rnn_model.eval()
    test_sequence = np.tile(test_sequence, (args.test_iteration, 1))
    test_sequence = autograd.Variable(
        torch.from_numpy(test_sequence).float()).to(self.device)
    # bookkeeping for beam search
    beam_set = [BeamState()]
    for num_iter in np.arange(0, args.test_iteration * test_sequence_length,
                              args.look_ahead):
      max_clusters = max([len(beam_state.mean_set) for beam_state in beam_set])
      look_ahead_seq = test_sequence[num_iter:  num_iter + args.look_ahead, :]
      look_ahead_seq_length = look_ahead_seq.shape[0]
      score_set = float(&#39;inf&#39;) * np.ones(
          np.append(
              args.beam_size, max_clusters + 1 + np.arange(
                  look_ahead_seq_length)))
      for beam_rank, beam_state in enumerate(beam_set):
        beam_score_set = self._calculate_score(beam_state, look_ahead_seq)
        score_set[beam_rank, :] = np.pad(
            beam_score_set,
            np.tile([[0, max_clusters - len(beam_state.mean_set)]],
                    (look_ahead_seq_length, 1)), &#39;constant&#39;,
            constant_values=float(&#39;inf&#39;))
      # find top scores
      score_ranked = np.sort(score_set, axis=None)
      score_ranked[score_ranked == float(&#39;inf&#39;)] = 0
      score_ranked = np.trim_zeros(score_ranked)
      idx_ranked = np.argsort(score_set, axis=None)
      updated_beam_set = []
      for new_beam_rank in range(
          np.min((len(score_ranked), args.beam_size))):
        total_idx = np.unravel_index(idx_ranked[new_beam_rank],
                                     score_set.shape)
        prev_beam_rank = total_idx[0]
        cluster_seq = total_idx[1:]
        updated_beam_state = self._update_beam_state(
            beam_set[prev_beam_rank], look_ahead_seq, cluster_seq)
        updated_beam_set.append(updated_beam_state)
      beam_set = updated_beam_set
    predicted_cluster_id = beam_set[0].trace[-test_sequence_length:]
    return predicted_cluster_id

  def predict(self, test_sequences, args):
    &#34;&#34;&#34;Predict labels for a single or many test sequences using UISRNN model.

    Args:
      test_sequences: Either a list of test sequences, or a single test
        sequence. Each test sequence is a 2-dim numpy array
        of real numbers. See `predict_single()` for details.
      args: Inference configurations. See `arguments.py` for details.

    Returns:
      predicted_cluster_ids: Predicted labels for test_sequences.

        1. if test_sequences is a list, predicted_cluster_ids will be a list
           of the same size, where each element being a 1-dim list of strings.
        2. if test_sequences is a single sequence, predicted_cluster_ids will
           be a 1-dim list of strings

    Raises:
      TypeError: If test_sequences is of wrong type.
    &#34;&#34;&#34;
    # check type
    if isinstance(test_sequences, np.ndarray):
      return self.predict_single(test_sequences, args)
    if isinstance(test_sequences, list):
      return [self.predict_single(test_sequence, args)
              for test_sequence in test_sequences]
    raise TypeError(&#39;test_sequences should be either a list or numpy array.&#39;)


def parallel_predict(model, test_sequences, args, num_processes=4):
  &#34;&#34;&#34;Run prediction in parallel using torch.multiprocessing.

  This is a beta feature. It makes prediction slower on CPU. But it&#39;s reported
  that it makes prediction faster on GPU.

  Args:
    model: instance of UISRNN model
    test_sequences: a list of test sequences, or a single test
      sequence. Each test sequence is a 2-dim numpy array
      of real numbers. See `predict_single()` for details.
    args: Inference configurations. See `arguments.py` for details.
    num_processes: number of parallel processes.

  Returns:
    a list of the same size as test_sequences, where each element
    being a 1-dim list of strings.

  Raises:
      TypeError: If test_sequences is of wrong type.
  &#34;&#34;&#34;
  if not isinstance(test_sequences, list):
    raise TypeError(&#39;test_sequences must be a list.&#39;)
  ctx = multiprocessing.get_context(&#39;forkserver&#39;)
  model.rnn_model.share_memory()
  pool = ctx.Pool(num_processes)
  results = pool.map(
      functools.partial(model.predict_single, args=args),
      test_sequences)
  pool.close()
  return results</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="uisrnn.uisrnn.parallel_predict"><code class="name flex">
<span>def <span class="ident">parallel_predict</span></span>(<span>model, test_sequences, args, num_processes=4)</span>
</code></dt>
<dd>
<section class="desc"><p>Run prediction in parallel using torch.multiprocessing.</p>
<p>This is a beta feature. It makes prediction slower on CPU. But it's reported
that it makes prediction faster on GPU.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>instance of UISRNN model</dd>
<dt><strong><code>test_sequences</code></strong></dt>
<dd>a list of test sequences, or a single test
sequence. Each test sequence is a 2-dim numpy array
of real numbers. See <code>predict_single()</code> for details.</dd>
<dt><strong><code>args</code></strong></dt>
<dd>Inference configurations. See <code>arguments.py</code> for details.</dd>
<dt><strong><code>num_processes</code></strong></dt>
<dd>number of parallel processes.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a list of the same size as test_sequences, where each element
being a 1-dim list of strings.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>TypeError</code></strong></dt>
<dd>If test_sequences is of wrong type.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def parallel_predict(model, test_sequences, args, num_processes=4):
  &#34;&#34;&#34;Run prediction in parallel using torch.multiprocessing.

  This is a beta feature. It makes prediction slower on CPU. But it&#39;s reported
  that it makes prediction faster on GPU.

  Args:
    model: instance of UISRNN model
    test_sequences: a list of test sequences, or a single test
      sequence. Each test sequence is a 2-dim numpy array
      of real numbers. See `predict_single()` for details.
    args: Inference configurations. See `arguments.py` for details.
    num_processes: number of parallel processes.

  Returns:
    a list of the same size as test_sequences, where each element
    being a 1-dim list of strings.

  Raises:
      TypeError: If test_sequences is of wrong type.
  &#34;&#34;&#34;
  if not isinstance(test_sequences, list):
    raise TypeError(&#39;test_sequences must be a list.&#39;)
  ctx = multiprocessing.get_context(&#39;forkserver&#39;)
  model.rnn_model.share_memory()
  pool = ctx.Pool(num_processes)
  results = pool.map(
      functools.partial(model.predict_single, args=args),
      test_sequences)
  pool.close()
  return results</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="uisrnn.uisrnn.BeamState"><code class="flex name class">
<span>class <span class="ident">BeamState</span></span>
</code></dt>
<dd>
<section class="desc"><p>Structure that contains necessary states for beam search.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class BeamState:
  &#34;&#34;&#34;Structure that contains necessary states for beam search.&#34;&#34;&#34;

  def __init__(self, source=None):
    if not source:
      self.mean_set = []
      self.hidden_set = []
      self.neg_likelihood = 0
      self.trace = []
      self.block_counts = []
    else:
      self.mean_set = source.mean_set.copy()
      self.hidden_set = source.hidden_set.copy()
      self.trace = source.trace.copy()
      self.block_counts = source.block_counts.copy()
      self.neg_likelihood = source.neg_likelihood

  def append(self, mean, hidden, cluster):
    &#34;&#34;&#34;Append new item to the BeamState.&#34;&#34;&#34;
    self.mean_set.append(mean.clone())
    self.hidden_set.append(hidden.clone())
    self.block_counts.append(1)
    self.trace.append(cluster)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="uisrnn.uisrnn.BeamState.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, source=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize self.
See help(type(self)) for accurate signature.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, source=None):
  if not source:
    self.mean_set = []
    self.hidden_set = []
    self.neg_likelihood = 0
    self.trace = []
    self.block_counts = []
  else:
    self.mean_set = source.mean_set.copy()
    self.hidden_set = source.hidden_set.copy()
    self.trace = source.trace.copy()
    self.block_counts = source.block_counts.copy()
    self.neg_likelihood = source.neg_likelihood</code></pre>
</details>
</dd>
<dt id="uisrnn.uisrnn.BeamState.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, mean, hidden, cluster)</span>
</code></dt>
<dd>
<section class="desc"><p>Append new item to the BeamState.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def append(self, mean, hidden, cluster):
  &#34;&#34;&#34;Append new item to the BeamState.&#34;&#34;&#34;
  self.mean_set.append(mean.clone())
  self.hidden_set.append(hidden.clone())
  self.block_counts.append(1)
  self.trace.append(cluster)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="uisrnn.uisrnn.CoreRNN"><code class="flex name class">
<span>class <span class="ident">CoreRNN</span></span>
<span>(</span><span><small>ancestors:</small> torch.nn.modules.module.Module)</span>
</code></dt>
<dd>
<section class="desc"><p>The core Recurent Neural Network used by UIS-RNN.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class CoreRNN(nn.Module):
  &#34;&#34;&#34;The core Recurent Neural Network used by UIS-RNN.&#34;&#34;&#34;

  def __init__(self, input_dim, hidden_size, depth, observation_dim, dropout=0):
    super(CoreRNN, self).__init__()
    self.hidden_size = hidden_size
    if depth &gt;= 2:
      self.gru = nn.GRU(input_dim, hidden_size, depth, dropout=dropout)
    else:
      self.gru = nn.GRU(input_dim, hidden_size, depth)
    self.linear_mean1 = nn.Linear(hidden_size, hidden_size)
    self.linear_mean2 = nn.Linear(hidden_size, observation_dim)

  def forward(self, input_seq, hidden=None):
    output_seq, hidden = self.gru(input_seq, hidden)
    if isinstance(output_seq, torch.nn.utils.rnn.PackedSequence):
      output_seq, _ = torch.nn.utils.rnn.pad_packed_sequence(
          output_seq, batch_first=False)
    mean = self.linear_mean2(F.relu(self.linear_mean1(output_seq)))
    return mean, hidden</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="uisrnn.uisrnn.CoreRNN.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, input_dim, hidden_size, depth, observation_dim, dropout=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize self.
See help(type(self)) for accurate signature.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, input_dim, hidden_size, depth, observation_dim, dropout=0):
  super(CoreRNN, self).__init__()
  self.hidden_size = hidden_size
  if depth &gt;= 2:
    self.gru = nn.GRU(input_dim, hidden_size, depth, dropout=dropout)
  else:
    self.gru = nn.GRU(input_dim, hidden_size, depth)
  self.linear_mean1 = nn.Linear(hidden_size, hidden_size)
  self.linear_mean2 = nn.Linear(hidden_size, observation_dim)</code></pre>
</details>
</dd>
<dt id="uisrnn.uisrnn.CoreRNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input_seq, hidden=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, input_seq, hidden=None):
  output_seq, hidden = self.gru(input_seq, hidden)
  if isinstance(output_seq, torch.nn.utils.rnn.PackedSequence):
    output_seq, _ = torch.nn.utils.rnn.pad_packed_sequence(
        output_seq, batch_first=False)
  mean = self.linear_mean2(F.relu(self.linear_mean1(output_seq)))
  return mean, hidden</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="uisrnn.uisrnn.UISRNN"><code class="flex name class">
<span>class <span class="ident">UISRNN</span></span>
</code></dt>
<dd>
<section class="desc"><p>Unbounded Interleaved-State Recurrent Neural Networks.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class UISRNN:
  &#34;&#34;&#34;Unbounded Interleaved-State Recurrent Neural Networks.&#34;&#34;&#34;

  def __init__(self, args):
    &#34;&#34;&#34;Construct the UISRNN object.

    Args:
      args: Model configurations. See `arguments.py` for details.
    &#34;&#34;&#34;
    self.observation_dim = args.observation_dim
    self.device = torch.device(
        &#39;cuda:0&#39; if (torch.cuda.is_available() and args.enable_cuda) else &#39;cpu&#39;)
    self.rnn_model = CoreRNN(self.observation_dim, args.rnn_hidden_size,
                             args.rnn_depth, self.observation_dim,
                             args.rnn_dropout).to(self.device)
    self.rnn_init_hidden = nn.Parameter(
        torch.zeros(args.rnn_depth, 1, args.rnn_hidden_size).to(self.device))
    # booleans indicating which variables are trainable
    self.estimate_sigma2 = (args.sigma2 is None)
    self.estimate_transition_bias = (args.transition_bias is None)
    # initial values of variables
    sigma2 = _INITIAL_SIGMA2_VALUE if self.estimate_sigma2 else args.sigma2
    self.sigma2 = nn.Parameter(
        sigma2 * torch.ones(self.observation_dim).to(self.device))
    self.transition_bias = args.transition_bias
    self.transition_bias_denominator = 0.0
    self.crp_alpha = args.crp_alpha
    self.logger = utils.Logger(args.verbosity)

  def _get_optimizer(self, optimizer, learning_rate):
    &#34;&#34;&#34;Get optimizer for UISRNN.

    Args:
      optimizer: string - name of the optimizer.
      learning_rate: - learning rate for the entire model.
        We do not customize learning rate for separate parts.

    Returns:
      a pytorch &#34;optim&#34; object
    &#34;&#34;&#34;
    params = [
        {
            &#39;params&#39;: self.rnn_model.parameters()
        },  # rnn parameters
        {
            &#39;params&#39;: self.rnn_init_hidden
        }  # rnn initial hidden state
    ]
    if self.estimate_sigma2:  # train sigma2
      params.append({
          &#39;params&#39;: self.sigma2
      })  # variance parameters
    assert optimizer == &#39;adam&#39;, &#39;Only adam optimizer is supported.&#39;
    return optim.Adam(params, lr=learning_rate)

  def save(self, filepath):
    &#34;&#34;&#34;Save the model to a file.

    Args:
      filepath: the path of the file.
    &#34;&#34;&#34;
    torch.save({
        &#39;rnn_state_dict&#39;: self.rnn_model.state_dict(),
        &#39;rnn_init_hidden&#39;: self.rnn_init_hidden.detach().cpu().numpy(),
        &#39;transition_bias&#39;: self.transition_bias,
        &#39;transition_bias_denominator&#39;: self.transition_bias_denominator,
        &#39;crp_alpha&#39;: self.crp_alpha,
        &#39;sigma2&#39;: self.sigma2.detach().cpu().numpy()}, filepath)

  def load(self, filepath):
    &#34;&#34;&#34;Load the model from a file.

    Args:
      filepath: the path of the file.
    &#34;&#34;&#34;
    var_dict = torch.load(filepath)
    self.rnn_model.load_state_dict(var_dict[&#39;rnn_state_dict&#39;])
    self.rnn_init_hidden = nn.Parameter(
        torch.from_numpy(var_dict[&#39;rnn_init_hidden&#39;]).to(self.device))
    self.transition_bias = float(var_dict[&#39;transition_bias&#39;])
    self.transition_bias_denominator = float(
        var_dict[&#39;transition_bias_denominator&#39;])
    self.crp_alpha = float(var_dict[&#39;crp_alpha&#39;])
    self.sigma2 = nn.Parameter(
        torch.from_numpy(var_dict[&#39;sigma2&#39;]).to(self.device))

    self.logger.print(
        3, &#39;Loaded model with transition_bias={}, crp_alpha={}, sigma2={}, &#39;
        &#39;rnn_init_hidden={}&#39;.format(
            self.transition_bias, self.crp_alpha, var_dict[&#39;sigma2&#39;],
            var_dict[&#39;rnn_init_hidden&#39;]))

  def fit_concatenated(self, train_sequence, train_cluster_id, args):
    &#34;&#34;&#34;Fit UISRNN model to concatenated sequence and cluster_id.

    Args:
      train_sequence: the training observation sequence, which is a
        2-dim numpy array of real numbers, of size `N * D`.

        - `N`: summation of lengths of all utterances.
        - `D`: observation dimension.

        For example,
      ```
      train_sequence =
      [[1.2 3.0 -4.1 6.0]    --&gt; an entry of speaker #0 from utterance &#39;iaaa&#39;
       [0.8 -1.1 0.4 0.5]    --&gt; an entry of speaker #1 from utterance &#39;iaaa&#39;
       [-0.2 1.0 3.8 5.7]    --&gt; an entry of speaker #0 from utterance &#39;iaaa&#39;
       [3.8 -0.1 1.5 2.3]    --&gt; an entry of speaker #0 from utterance &#39;ibbb&#39;
       [1.2 1.4 3.6 -2.7]]   --&gt; an entry of speaker #0 from utterance &#39;ibbb&#39;
      ```
        Here `N=5`, `D=4`.

        We concatenate all training utterances into this single sequence.
      train_cluster_id: the speaker id sequence, which is 1-dim list or
        numpy array of strings, of size `N`.
        For example,
      ```
      train_cluster_id =
        [&#39;iaaa_0&#39;, &#39;iaaa_1&#39;, &#39;iaaa_0&#39;, &#39;ibbb_0&#39;, &#39;ibbb_0&#39;]
      ```
        &#39;iaaa_0&#39; means the entry belongs to speaker #0 in utterance &#39;iaaa&#39;.

        Note that the order of entries within an utterance are preserved,
        and all utterances are simply concatenated together.
      args: Training configurations. See `arguments.py` for details.

    Raises:
      TypeError: If train_sequence or train_cluster_id is of wrong type.
      ValueError: If train_sequence or train_cluster_id has wrong dimension.
    &#34;&#34;&#34;
    # check type
    if (not isinstance(train_sequence, np.ndarray) or
        train_sequence.dtype != float):
      raise TypeError(&#39;train_sequence should be a numpy array of float type.&#39;)
    if isinstance(train_cluster_id, list):
      train_cluster_id = np.array(train_cluster_id)
    if (not isinstance(train_cluster_id, np.ndarray) or
        not train_cluster_id.dtype.name.startswith((&#39;str&#39;, &#39;unicode&#39;))):
      raise TypeError(&#39;train_cluster_id type be a numpy array of strings.&#39;)
    # check dimension
    if train_sequence.ndim != 2:
      raise ValueError(&#39;train_sequence must be 2-dim array.&#39;)
    if train_cluster_id.ndim != 1:
      raise ValueError(&#39;train_cluster_id must be 1-dim array.&#39;)
    # check length and size
    train_total_length, observation_dim = train_sequence.shape
    if observation_dim != self.observation_dim:
      raise ValueError(&#39;train_sequence does not match the dimension specified &#39;
                       &#39;by args.observation_dim.&#39;)
    if train_total_length != len(train_cluster_id):
      raise ValueError(&#39;train_sequence length is not equal to &#39;
                       &#39;train_cluster_id length.&#39;)

    self.rnn_model.train()
    optimizer = self._get_optimizer(optimizer=args.optimizer,
                                    learning_rate=args.learning_rate)

    sub_sequences, seq_lengths = utils.resize_sequence(
        sequence=train_sequence,
        cluster_id=train_cluster_id,
        num_permutations=args.num_permutations)

    # For batch learning, pack the entire dataset.
    if args.batch_size is None:
      packed_train_sequence, rnn_truth = utils.pack_sequence(
          sub_sequences,
          seq_lengths,
          args.batch_size,
          self.observation_dim,
          self.device)
    train_loss = []
    for num_iter in range(args.train_iteration):
      optimizer.zero_grad()
      # For online learning, pack a subset in each iteration.
      if args.batch_size is not None:
        packed_train_sequence, rnn_truth = utils.pack_sequence(
            sub_sequences,
            seq_lengths,
            args.batch_size,
            self.observation_dim,
            self.device)
      hidden = self.rnn_init_hidden.repeat(1, args.batch_size, 1)
      mean, _ = self.rnn_model(packed_train_sequence, hidden)
      # use mean to predict
      mean = torch.cumsum(mean, dim=0)
      mean_size = mean.size()
      mean = torch.mm(
          torch.diag(
              1.0 / torch.arange(1, mean_size[0] + 1).float().to(self.device)),
          mean.view(mean_size[0], -1))
      mean = mean.view(mean_size)

      # Likelihood part.
      loss1 = loss_func.weighted_mse_loss(
          input_tensor=(rnn_truth != 0).float() * mean[:-1, :, :],
          target_tensor=rnn_truth,
          weight=1 / (2 * self.sigma2))

      # Sigma2 prior part.
      weight = (((rnn_truth != 0).float() * mean[:-1, :, :] - rnn_truth)
                ** 2).view(-1, observation_dim)
      num_non_zero = torch.sum((weight != 0).float(), dim=0).squeeze()
      loss2 = loss_func.sigma2_prior_loss(
          num_non_zero, args.sigma_alpha, args.sigma_beta, self.sigma2)

      # Regularization part.
      loss3 = loss_func.regularization_loss(
          self.rnn_model.parameters(), args.regularization_weight)

      loss = loss1 + loss2 + loss3
      loss.backward()
      nn.utils.clip_grad_norm_(self.rnn_model.parameters(), args.grad_max_norm)
      optimizer.step()
      # avoid numerical issues
      self.sigma2.data.clamp_(min=1e-6)

      if (np.remainder(num_iter, 10) == 0 or
          num_iter == args.train_iteration - 1):
        self.logger.print(
            2,
            &#39;Iter: {:d}  \t&#39;
            &#39;Training Loss: {:.4f}    \n&#39;
            &#39;    Negative Log Likelihood: {:.4f}\t&#39;
            &#39;Sigma2 Prior: {:.4f}\t&#39;
            &#39;Regularization: {:.4f}&#39;.format(
                num_iter,
                float(loss.data),
                float(loss1.data),
                float(loss2.data),
                float(loss3.data)))
      train_loss.append(float(loss1.data))  # only save the likelihood part
    self.logger.print(
        1, &#39;Done training with {} iterations&#39;.format(args.train_iteration))

  def fit(self, train_sequences, train_cluster_ids, args):
    &#34;&#34;&#34;Fit UISRNN model.

    Args:
      train_sequences: Either a list of training sequences, or a single
        concatenated training sequence:

        1. train_sequences is list, and each element is a 2-dim numpy array
           of real numbers, of size: `length * D`.
           The length varies among different sequences, but the D is the same.
           In speaker diarization, each sequence is the sequence of speaker
           embeddings of one utterance.
        2. train_sequences is a single concatenated sequence, which is a
           2-dim numpy array of real numbers. See `fit_concatenated()`
           for more details.
      train_cluster_ids: Ground truth labels for train_sequences:

        1. if train_sequences is a list, this must also be a list of the same
           size, each element being a 1-dim list or numpy array of strings.
        2. if train_sequences is a single concatenated sequence, this
           must also be the concatenated 1-dim list or numpy array of strings
      args: Training configurations. See `arguments.py` for details.

    Raises:
      TypeError: If train_sequences or train_cluster_ids is of wrong type.
    &#34;&#34;&#34;
    if isinstance(train_sequences, np.ndarray):
      # train_sequences is already the concatenated sequence
      if self.estimate_transition_bias:
        # see issue #55: https://github.com/google/uis-rnn/issues/55
        self.logger.print(
            2,
            &#39;Warning: transition_bias cannot be correctly estimated from a &#39;
            &#39;concatenated sequence; train_sequences will be treated as a &#39;
            &#39;single sequence. This can lead to inaccurate estimation of &#39;
            &#39;transition_bias. Please, consider estimating transition_bias &#39;
            &#39;before concatenating the sequences and passing it as argument.&#39;)
      train_sequences = [train_sequences]
      train_cluster_ids = [train_cluster_ids]
    elif isinstance(train_sequences, list):
      # train_sequences is a list of un-concatenated sequences
      # we will concatenate it later, after estimating transition_bias
      pass
    else:
      raise TypeError(&#39;train_sequences must be a list or numpy.ndarray&#39;)

    # estimate transition_bias
    if self.estimate_transition_bias:
      (transition_bias,
       transition_bias_denominator) = utils.estimate_transition_bias(
           train_cluster_ids)
      # set or update transition_bias
      if self.transition_bias is None:
        self.transition_bias = transition_bias
        self.transition_bias_denominator = transition_bias_denominator
      else:
        self.transition_bias = (
            self.transition_bias * self.transition_bias_denominator +
            transition_bias * transition_bias_denominator) / (
                self.transition_bias_denominator + transition_bias_denominator)
        self.transition_bias_denominator += transition_bias_denominator

    # concatenate train_sequences
    (concatenated_train_sequence,
     concatenated_train_cluster_id) = utils.concatenate_training_data(
         train_sequences,
         train_cluster_ids,
         args.enforce_cluster_id_uniqueness,
         True)

    self.fit_concatenated(
        concatenated_train_sequence, concatenated_train_cluster_id, args)

  def _update_beam_state(self, beam_state, look_ahead_seq, cluster_seq):
    &#34;&#34;&#34;Update a beam state given a look ahead sequence and known cluster
    assignments.

    Args:
      beam_state: A BeamState object.
      look_ahead_seq: Look ahead sequence, size: look_ahead*D.
        look_ahead: number of step to look ahead in the beam search.
        D: observation dimension
      cluster_seq: Cluster assignment sequence for look_ahead_seq.

    Returns:
      new_beam_state: An updated BeamState object.
    &#34;&#34;&#34;

    loss = 0
    new_beam_state = BeamState(beam_state)
    for sub_idx, cluster in enumerate(cluster_seq):
      if cluster &gt; len(new_beam_state.mean_set):  # invalid trace
        new_beam_state.neg_likelihood = float(&#39;inf&#39;)
        break
      elif cluster &lt; len(new_beam_state.mean_set):  # existing cluster
        last_cluster = new_beam_state.trace[-1]
        loss = loss_func.weighted_mse_loss(
            input_tensor=torch.squeeze(new_beam_state.mean_set[cluster]),
            target_tensor=look_ahead_seq[sub_idx, :],
            weight=1 / (2 * self.sigma2)).cpu().detach().numpy()
        if cluster == last_cluster:
          loss -= np.log(1 - self.transition_bias)
        else:
          loss -= np.log(self.transition_bias) + np.log(
              new_beam_state.block_counts[cluster]) - np.log(
                  sum(new_beam_state.block_counts) + self.crp_alpha)
        # update new mean and new hidden
        mean, hidden = self.rnn_model(
            look_ahead_seq[sub_idx, :].unsqueeze(0).unsqueeze(0),
            new_beam_state.hidden_set[cluster])
        new_beam_state.mean_set[cluster] = (new_beam_state.mean_set[cluster]*(
            (np.array(new_beam_state.trace) == cluster).sum() -
            1).astype(float) + mean.clone()) / (
                np.array(new_beam_state.trace) == cluster).sum().astype(
                    float)  # use mean to predict
        new_beam_state.hidden_set[cluster] = hidden.clone()
        if cluster != last_cluster:
          new_beam_state.block_counts[cluster] += 1
        new_beam_state.trace.append(cluster)
      else:  # new cluster
        init_input = autograd.Variable(
            torch.zeros(self.observation_dim)
        ).unsqueeze(0).unsqueeze(0).to(self.device)
        mean, hidden = self.rnn_model(init_input,
                                      self.rnn_init_hidden)
        loss = loss_func.weighted_mse_loss(
            input_tensor=torch.squeeze(mean),
            target_tensor=look_ahead_seq[sub_idx, :],
            weight=1 / (2 * self.sigma2)).cpu().detach().numpy()
        loss -= np.log(self.transition_bias) + np.log(
            self.crp_alpha) - np.log(
                sum(new_beam_state.block_counts) + self.crp_alpha)
        # update new min and new hidden
        mean, hidden = self.rnn_model(
            look_ahead_seq[sub_idx, :].unsqueeze(0).unsqueeze(0),
            hidden)
        new_beam_state.append(mean, hidden, cluster)
      new_beam_state.neg_likelihood += loss
    return new_beam_state

  def _calculate_score(self, beam_state, look_ahead_seq):
    &#34;&#34;&#34;Calculate negative log likelihoods for all possible state allocations
       of a look ahead sequence, according to the current beam state.

    Args:
      beam_state: A BeamState object.
      look_ahead_seq: Look ahead sequence, size: look_ahead*D.
        look_ahead: number of step to look ahead in the beam search.
        D: observation dimension

    Returns:
      beam_score_set: a set of scores for each possible state allocation.
    &#34;&#34;&#34;

    look_ahead, _ = look_ahead_seq.shape
    beam_num_clusters = len(beam_state.mean_set)
    beam_score_set = float(&#39;inf&#39;) * np.ones(
        beam_num_clusters + 1 + np.arange(look_ahead))
    for cluster_seq, _ in np.ndenumerate(beam_score_set):
      updated_beam_state = self._update_beam_state(beam_state,
                                                   look_ahead_seq, cluster_seq)
      beam_score_set[cluster_seq] = updated_beam_state.neg_likelihood
    return beam_score_set

  def predict_single(self, test_sequence, args):
    &#34;&#34;&#34;Predict labels for a single test sequence using UISRNN model.

    Args:
      test_sequence: the test observation sequence, which is 2-dim numpy array
        of real numbers, of size `N * D`.

        - `N`: length of one test utterance.
        - `D` : observation dimension.

        For example:
      ```
      test_sequence =
      [[2.2 -1.0 3.0 5.6]    --&gt; 1st entry of utterance &#39;iccc&#39;
       [0.5 1.8 -3.2 0.4]    --&gt; 2nd entry of utterance &#39;iccc&#39;
       [-2.2 5.0 1.8 3.7]    --&gt; 3rd entry of utterance &#39;iccc&#39;
       [-3.8 0.1 1.4 3.3]    --&gt; 4th entry of utterance &#39;iccc&#39;
       [0.1 2.7 3.5 -1.7]]   --&gt; 5th entry of utterance &#39;iccc&#39;
      ```
        Here `N=5`, `D=4`.
      args: Inference configurations. See `arguments.py` for details.

    Returns:
      predicted_cluster_id: predicted speaker id sequence, which is
        an array of integers, of size `N`.
        For example, `predicted_cluster_id = [0, 1, 0, 0, 1]`

    Raises:
      TypeError: If test_sequence is of wrong type.
      ValueError: If test_sequence has wrong dimension.
    &#34;&#34;&#34;
    # check type
    if (not isinstance(test_sequence, np.ndarray) or
        test_sequence.dtype != float):
      raise TypeError(&#39;test_sequence should be a numpy array of float type.&#39;)
    # check dimension
    if test_sequence.ndim != 2:
      raise ValueError(&#39;test_sequence must be 2-dim array.&#39;)
    # check size
    test_sequence_length, observation_dim = test_sequence.shape
    if observation_dim != self.observation_dim:
      raise ValueError(&#39;test_sequence does not match the dimension specified &#39;
                       &#39;by args.observation_dim.&#39;)

    self.rnn_model.eval()
    test_sequence = np.tile(test_sequence, (args.test_iteration, 1))
    test_sequence = autograd.Variable(
        torch.from_numpy(test_sequence).float()).to(self.device)
    # bookkeeping for beam search
    beam_set = [BeamState()]
    for num_iter in np.arange(0, args.test_iteration * test_sequence_length,
                              args.look_ahead):
      max_clusters = max([len(beam_state.mean_set) for beam_state in beam_set])
      look_ahead_seq = test_sequence[num_iter:  num_iter + args.look_ahead, :]
      look_ahead_seq_length = look_ahead_seq.shape[0]
      score_set = float(&#39;inf&#39;) * np.ones(
          np.append(
              args.beam_size, max_clusters + 1 + np.arange(
                  look_ahead_seq_length)))
      for beam_rank, beam_state in enumerate(beam_set):
        beam_score_set = self._calculate_score(beam_state, look_ahead_seq)
        score_set[beam_rank, :] = np.pad(
            beam_score_set,
            np.tile([[0, max_clusters - len(beam_state.mean_set)]],
                    (look_ahead_seq_length, 1)), &#39;constant&#39;,
            constant_values=float(&#39;inf&#39;))
      # find top scores
      score_ranked = np.sort(score_set, axis=None)
      score_ranked[score_ranked == float(&#39;inf&#39;)] = 0
      score_ranked = np.trim_zeros(score_ranked)
      idx_ranked = np.argsort(score_set, axis=None)
      updated_beam_set = []
      for new_beam_rank in range(
          np.min((len(score_ranked), args.beam_size))):
        total_idx = np.unravel_index(idx_ranked[new_beam_rank],
                                     score_set.shape)
        prev_beam_rank = total_idx[0]
        cluster_seq = total_idx[1:]
        updated_beam_state = self._update_beam_state(
            beam_set[prev_beam_rank], look_ahead_seq, cluster_seq)
        updated_beam_set.append(updated_beam_state)
      beam_set = updated_beam_set
    predicted_cluster_id = beam_set[0].trace[-test_sequence_length:]
    return predicted_cluster_id

  def predict(self, test_sequences, args):
    &#34;&#34;&#34;Predict labels for a single or many test sequences using UISRNN model.

    Args:
      test_sequences: Either a list of test sequences, or a single test
        sequence. Each test sequence is a 2-dim numpy array
        of real numbers. See `predict_single()` for details.
      args: Inference configurations. See `arguments.py` for details.

    Returns:
      predicted_cluster_ids: Predicted labels for test_sequences.

        1. if test_sequences is a list, predicted_cluster_ids will be a list
           of the same size, where each element being a 1-dim list of strings.
        2. if test_sequences is a single sequence, predicted_cluster_ids will
           be a 1-dim list of strings

    Raises:
      TypeError: If test_sequences is of wrong type.
    &#34;&#34;&#34;
    # check type
    if isinstance(test_sequences, np.ndarray):
      return self.predict_single(test_sequences, args)
    if isinstance(test_sequences, list):
      return [self.predict_single(test_sequence, args)
              for test_sequence in test_sequences]
    raise TypeError(&#39;test_sequences should be either a list or numpy array.&#39;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="uisrnn.uisrnn.UISRNN.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, args)</span>
</code></dt>
<dd>
<section class="desc"><p>Construct the UISRNN object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>args</code></strong></dt>
<dd>Model configurations. See <code>arguments.py</code> for details.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, args):
  &#34;&#34;&#34;Construct the UISRNN object.

  Args:
    args: Model configurations. See `arguments.py` for details.
  &#34;&#34;&#34;
  self.observation_dim = args.observation_dim
  self.device = torch.device(
      &#39;cuda:0&#39; if (torch.cuda.is_available() and args.enable_cuda) else &#39;cpu&#39;)
  self.rnn_model = CoreRNN(self.observation_dim, args.rnn_hidden_size,
                           args.rnn_depth, self.observation_dim,
                           args.rnn_dropout).to(self.device)
  self.rnn_init_hidden = nn.Parameter(
      torch.zeros(args.rnn_depth, 1, args.rnn_hidden_size).to(self.device))
  # booleans indicating which variables are trainable
  self.estimate_sigma2 = (args.sigma2 is None)
  self.estimate_transition_bias = (args.transition_bias is None)
  # initial values of variables
  sigma2 = _INITIAL_SIGMA2_VALUE if self.estimate_sigma2 else args.sigma2
  self.sigma2 = nn.Parameter(
      sigma2 * torch.ones(self.observation_dim).to(self.device))
  self.transition_bias = args.transition_bias
  self.transition_bias_denominator = 0.0
  self.crp_alpha = args.crp_alpha
  self.logger = utils.Logger(args.verbosity)</code></pre>
</details>
</dd>
<dt id="uisrnn.uisrnn.UISRNN.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, train_sequences, train_cluster_ids, args)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit UISRNN model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>train_sequences</code></strong></dt>
<dd>
<p>Either a list of training sequences, or a single
concatenated training sequence:</p>
<ol>
<li>train_sequences is list, and each element is a 2-dim numpy array
of real numbers, of size: <code>length * D</code>.
The length varies among different sequences, but the D is the same.
In speaker diarization, each sequence is the sequence of speaker
embeddings of one utterance.</li>
<li>train_sequences is a single concatenated sequence, which is a
2-dim numpy array of real numbers. See <code>fit_concatenated()</code>
for more details.</li>
</ol>
</dd>
<dt><strong><code>train_cluster_ids</code></strong></dt>
<dd>
<p>Ground truth labels for train_sequences:</p>
<ol>
<li>if train_sequences is a list, this must also be a list of the same
size, each element being a 1-dim list or numpy array of strings.</li>
<li>if train_sequences is a single concatenated sequence, this
must also be the concatenated 1-dim list or numpy array of strings</li>
</ol>
</dd>
<dt><strong><code>args</code></strong></dt>
<dd>Training configurations. See <code>arguments.py</code> for details.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>TypeError</code></strong></dt>
<dd>If train_sequences or train_cluster_ids is of wrong type.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit(self, train_sequences, train_cluster_ids, args):
  &#34;&#34;&#34;Fit UISRNN model.

  Args:
    train_sequences: Either a list of training sequences, or a single
      concatenated training sequence:

      1. train_sequences is list, and each element is a 2-dim numpy array
         of real numbers, of size: `length * D`.
         The length varies among different sequences, but the D is the same.
         In speaker diarization, each sequence is the sequence of speaker
         embeddings of one utterance.
      2. train_sequences is a single concatenated sequence, which is a
         2-dim numpy array of real numbers. See `fit_concatenated()`
         for more details.
    train_cluster_ids: Ground truth labels for train_sequences:

      1. if train_sequences is a list, this must also be a list of the same
         size, each element being a 1-dim list or numpy array of strings.
      2. if train_sequences is a single concatenated sequence, this
         must also be the concatenated 1-dim list or numpy array of strings
    args: Training configurations. See `arguments.py` for details.

  Raises:
    TypeError: If train_sequences or train_cluster_ids is of wrong type.
  &#34;&#34;&#34;
  if isinstance(train_sequences, np.ndarray):
    # train_sequences is already the concatenated sequence
    if self.estimate_transition_bias:
      # see issue #55: https://github.com/google/uis-rnn/issues/55
      self.logger.print(
          2,
          &#39;Warning: transition_bias cannot be correctly estimated from a &#39;
          &#39;concatenated sequence; train_sequences will be treated as a &#39;
          &#39;single sequence. This can lead to inaccurate estimation of &#39;
          &#39;transition_bias. Please, consider estimating transition_bias &#39;
          &#39;before concatenating the sequences and passing it as argument.&#39;)
    train_sequences = [train_sequences]
    train_cluster_ids = [train_cluster_ids]
  elif isinstance(train_sequences, list):
    # train_sequences is a list of un-concatenated sequences
    # we will concatenate it later, after estimating transition_bias
    pass
  else:
    raise TypeError(&#39;train_sequences must be a list or numpy.ndarray&#39;)

  # estimate transition_bias
  if self.estimate_transition_bias:
    (transition_bias,
     transition_bias_denominator) = utils.estimate_transition_bias(
         train_cluster_ids)
    # set or update transition_bias
    if self.transition_bias is None:
      self.transition_bias = transition_bias
      self.transition_bias_denominator = transition_bias_denominator
    else:
      self.transition_bias = (
          self.transition_bias * self.transition_bias_denominator +
          transition_bias * transition_bias_denominator) / (
              self.transition_bias_denominator + transition_bias_denominator)
      self.transition_bias_denominator += transition_bias_denominator

  # concatenate train_sequences
  (concatenated_train_sequence,
   concatenated_train_cluster_id) = utils.concatenate_training_data(
       train_sequences,
       train_cluster_ids,
       args.enforce_cluster_id_uniqueness,
       True)

  self.fit_concatenated(
      concatenated_train_sequence, concatenated_train_cluster_id, args)</code></pre>
</details>
</dd>
<dt id="uisrnn.uisrnn.UISRNN.fit_concatenated"><code class="name flex">
<span>def <span class="ident">fit_concatenated</span></span>(<span>self, train_sequence, train_cluster_id, args)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit UISRNN model to concatenated sequence and cluster_id.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>train_sequence</code></strong></dt>
<dd>
<p>the training observation sequence, which is a
2-dim numpy array of real numbers, of size <code>N * D</code>.</p>
<ul>
<li><code>N</code>: summation of lengths of all utterances.</li>
<li><code>D</code>: observation dimension.</li>
</ul>
<p>For example,</p>
</dd>
</dl>
<pre><code>train_sequence =
[[1.2 3.0 -4.1 6.0]    --&gt; an entry of speaker #0 from utterance 'iaaa'
 [0.8 -1.1 0.4 0.5]    --&gt; an entry of speaker #1 from utterance 'iaaa'
 [-0.2 1.0 3.8 5.7]    --&gt; an entry of speaker #0 from utterance 'iaaa'
 [3.8 -0.1 1.5 2.3]    --&gt; an entry of speaker #0 from utterance 'ibbb'
 [1.2 1.4 3.6 -2.7]]   --&gt; an entry of speaker #0 from utterance 'ibbb'
</code></pre>
<p>Here <code>N=5</code>, <code>D=4</code>.</p>
<dl>
<dt>We concatenate all training utterances into this single sequence.</dt>
<dt><strong><code>train_cluster_id</code></strong></dt>
<dd>the speaker id sequence, which is 1-dim list or
numpy array of strings, of size <code>N</code>.
For example,</dd>
</dl>
<pre><code>train_cluster_id =
  ['iaaa_0', 'iaaa_1', 'iaaa_0', 'ibbb_0', 'ibbb_0']
</code></pre>
<p>'iaaa_0' means the entry belongs to speaker #0 in utterance 'iaaa'.</p>
<dl>
<dt>Note that the order of entries within an utterance are preserved,</dt>
<dt>and all utterances are simply concatenated together.</dt>
<dt><strong><code>args</code></strong></dt>
<dd>Training configurations. See <code>arguments.py</code> for details.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>TypeError</code></strong></dt>
<dd>If train_sequence or train_cluster_id is of wrong type.</dd>
<dt><strong><code>ValueError</code></strong></dt>
<dd>If train_sequence or train_cluster_id has wrong dimension.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit_concatenated(self, train_sequence, train_cluster_id, args):
  &#34;&#34;&#34;Fit UISRNN model to concatenated sequence and cluster_id.

  Args:
    train_sequence: the training observation sequence, which is a
      2-dim numpy array of real numbers, of size `N * D`.

      - `N`: summation of lengths of all utterances.
      - `D`: observation dimension.

      For example,
    ```
    train_sequence =
    [[1.2 3.0 -4.1 6.0]    --&gt; an entry of speaker #0 from utterance &#39;iaaa&#39;
     [0.8 -1.1 0.4 0.5]    --&gt; an entry of speaker #1 from utterance &#39;iaaa&#39;
     [-0.2 1.0 3.8 5.7]    --&gt; an entry of speaker #0 from utterance &#39;iaaa&#39;
     [3.8 -0.1 1.5 2.3]    --&gt; an entry of speaker #0 from utterance &#39;ibbb&#39;
     [1.2 1.4 3.6 -2.7]]   --&gt; an entry of speaker #0 from utterance &#39;ibbb&#39;
    ```
      Here `N=5`, `D=4`.

      We concatenate all training utterances into this single sequence.
    train_cluster_id: the speaker id sequence, which is 1-dim list or
      numpy array of strings, of size `N`.
      For example,
    ```
    train_cluster_id =
      [&#39;iaaa_0&#39;, &#39;iaaa_1&#39;, &#39;iaaa_0&#39;, &#39;ibbb_0&#39;, &#39;ibbb_0&#39;]
    ```
      &#39;iaaa_0&#39; means the entry belongs to speaker #0 in utterance &#39;iaaa&#39;.

      Note that the order of entries within an utterance are preserved,
      and all utterances are simply concatenated together.
    args: Training configurations. See `arguments.py` for details.

  Raises:
    TypeError: If train_sequence or train_cluster_id is of wrong type.
    ValueError: If train_sequence or train_cluster_id has wrong dimension.
  &#34;&#34;&#34;
  # check type
  if (not isinstance(train_sequence, np.ndarray) or
      train_sequence.dtype != float):
    raise TypeError(&#39;train_sequence should be a numpy array of float type.&#39;)
  if isinstance(train_cluster_id, list):
    train_cluster_id = np.array(train_cluster_id)
  if (not isinstance(train_cluster_id, np.ndarray) or
      not train_cluster_id.dtype.name.startswith((&#39;str&#39;, &#39;unicode&#39;))):
    raise TypeError(&#39;train_cluster_id type be a numpy array of strings.&#39;)
  # check dimension
  if train_sequence.ndim != 2:
    raise ValueError(&#39;train_sequence must be 2-dim array.&#39;)
  if train_cluster_id.ndim != 1:
    raise ValueError(&#39;train_cluster_id must be 1-dim array.&#39;)
  # check length and size
  train_total_length, observation_dim = train_sequence.shape
  if observation_dim != self.observation_dim:
    raise ValueError(&#39;train_sequence does not match the dimension specified &#39;
                     &#39;by args.observation_dim.&#39;)
  if train_total_length != len(train_cluster_id):
    raise ValueError(&#39;train_sequence length is not equal to &#39;
                     &#39;train_cluster_id length.&#39;)

  self.rnn_model.train()
  optimizer = self._get_optimizer(optimizer=args.optimizer,
                                  learning_rate=args.learning_rate)

  sub_sequences, seq_lengths = utils.resize_sequence(
      sequence=train_sequence,
      cluster_id=train_cluster_id,
      num_permutations=args.num_permutations)

  # For batch learning, pack the entire dataset.
  if args.batch_size is None:
    packed_train_sequence, rnn_truth = utils.pack_sequence(
        sub_sequences,
        seq_lengths,
        args.batch_size,
        self.observation_dim,
        self.device)
  train_loss = []
  for num_iter in range(args.train_iteration):
    optimizer.zero_grad()
    # For online learning, pack a subset in each iteration.
    if args.batch_size is not None:
      packed_train_sequence, rnn_truth = utils.pack_sequence(
          sub_sequences,
          seq_lengths,
          args.batch_size,
          self.observation_dim,
          self.device)
    hidden = self.rnn_init_hidden.repeat(1, args.batch_size, 1)
    mean, _ = self.rnn_model(packed_train_sequence, hidden)
    # use mean to predict
    mean = torch.cumsum(mean, dim=0)
    mean_size = mean.size()
    mean = torch.mm(
        torch.diag(
            1.0 / torch.arange(1, mean_size[0] + 1).float().to(self.device)),
        mean.view(mean_size[0], -1))
    mean = mean.view(mean_size)

    # Likelihood part.
    loss1 = loss_func.weighted_mse_loss(
        input_tensor=(rnn_truth != 0).float() * mean[:-1, :, :],
        target_tensor=rnn_truth,
        weight=1 / (2 * self.sigma2))

    # Sigma2 prior part.
    weight = (((rnn_truth != 0).float() * mean[:-1, :, :] - rnn_truth)
              ** 2).view(-1, observation_dim)
    num_non_zero = torch.sum((weight != 0).float(), dim=0).squeeze()
    loss2 = loss_func.sigma2_prior_loss(
        num_non_zero, args.sigma_alpha, args.sigma_beta, self.sigma2)

    # Regularization part.
    loss3 = loss_func.regularization_loss(
        self.rnn_model.parameters(), args.regularization_weight)

    loss = loss1 + loss2 + loss3
    loss.backward()
    nn.utils.clip_grad_norm_(self.rnn_model.parameters(), args.grad_max_norm)
    optimizer.step()
    # avoid numerical issues
    self.sigma2.data.clamp_(min=1e-6)

    if (np.remainder(num_iter, 10) == 0 or
        num_iter == args.train_iteration - 1):
      self.logger.print(
          2,
          &#39;Iter: {:d}  \t&#39;
          &#39;Training Loss: {:.4f}    \n&#39;
          &#39;    Negative Log Likelihood: {:.4f}\t&#39;
          &#39;Sigma2 Prior: {:.4f}\t&#39;
          &#39;Regularization: {:.4f}&#39;.format(
              num_iter,
              float(loss.data),
              float(loss1.data),
              float(loss2.data),
              float(loss3.data)))
    train_loss.append(float(loss1.data))  # only save the likelihood part
  self.logger.print(
      1, &#39;Done training with {} iterations&#39;.format(args.train_iteration))</code></pre>
</details>
</dd>
<dt id="uisrnn.uisrnn.UISRNN.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filepath)</span>
</code></dt>
<dd>
<section class="desc"><p>Load the model from a file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>the path of the file.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load(self, filepath):
  &#34;&#34;&#34;Load the model from a file.

  Args:
    filepath: the path of the file.
  &#34;&#34;&#34;
  var_dict = torch.load(filepath)
  self.rnn_model.load_state_dict(var_dict[&#39;rnn_state_dict&#39;])
  self.rnn_init_hidden = nn.Parameter(
      torch.from_numpy(var_dict[&#39;rnn_init_hidden&#39;]).to(self.device))
  self.transition_bias = float(var_dict[&#39;transition_bias&#39;])
  self.transition_bias_denominator = float(
      var_dict[&#39;transition_bias_denominator&#39;])
  self.crp_alpha = float(var_dict[&#39;crp_alpha&#39;])
  self.sigma2 = nn.Parameter(
      torch.from_numpy(var_dict[&#39;sigma2&#39;]).to(self.device))

  self.logger.print(
      3, &#39;Loaded model with transition_bias={}, crp_alpha={}, sigma2={}, &#39;
      &#39;rnn_init_hidden={}&#39;.format(
          self.transition_bias, self.crp_alpha, var_dict[&#39;sigma2&#39;],
          var_dict[&#39;rnn_init_hidden&#39;]))</code></pre>
</details>
</dd>
<dt id="uisrnn.uisrnn.UISRNN.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, test_sequences, args)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict labels for a single or many test sequences using UISRNN model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>test_sequences</code></strong></dt>
<dd>Either a list of test sequences, or a single test
sequence. Each test sequence is a 2-dim numpy array
of real numbers. See <code>predict_single()</code> for details.</dd>
<dt><strong><code>args</code></strong></dt>
<dd>Inference configurations. See <code>arguments.py</code> for details.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>predicted_cluster_ids</code></strong></dt>
<dd>
<p>Predicted labels for test_sequences.</p>
<ol>
<li>if test_sequences is a list, predicted_cluster_ids will be a list
of the same size, where each element being a 1-dim list of strings.</li>
<li>if test_sequences is a single sequence, predicted_cluster_ids will
be a 1-dim list of strings</li>
</ol>
</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>TypeError</code></strong></dt>
<dd>If test_sequences is of wrong type.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def predict(self, test_sequences, args):
  &#34;&#34;&#34;Predict labels for a single or many test sequences using UISRNN model.

  Args:
    test_sequences: Either a list of test sequences, or a single test
      sequence. Each test sequence is a 2-dim numpy array
      of real numbers. See `predict_single()` for details.
    args: Inference configurations. See `arguments.py` for details.

  Returns:
    predicted_cluster_ids: Predicted labels for test_sequences.

      1. if test_sequences is a list, predicted_cluster_ids will be a list
         of the same size, where each element being a 1-dim list of strings.
      2. if test_sequences is a single sequence, predicted_cluster_ids will
         be a 1-dim list of strings

  Raises:
    TypeError: If test_sequences is of wrong type.
  &#34;&#34;&#34;
  # check type
  if isinstance(test_sequences, np.ndarray):
    return self.predict_single(test_sequences, args)
  if isinstance(test_sequences, list):
    return [self.predict_single(test_sequence, args)
            for test_sequence in test_sequences]
  raise TypeError(&#39;test_sequences should be either a list or numpy array.&#39;)</code></pre>
</details>
</dd>
<dt id="uisrnn.uisrnn.UISRNN.predict_single"><code class="name flex">
<span>def <span class="ident">predict_single</span></span>(<span>self, test_sequence, args)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict labels for a single test sequence using UISRNN model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>test_sequence</code></strong></dt>
<dd>
<p>the test observation sequence, which is 2-dim numpy array
of real numbers, of size <code>N * D</code>.</p>
<ul>
<li><code>N</code>: length of one test utterance.</li>
<li><code>D</code> : observation dimension.</li>
</ul>
<p>For example:</p>
</dd>
</dl>
<pre><code>test_sequence =
[[2.2 -1.0 3.0 5.6]    --&gt; 1st entry of utterance 'iccc'
 [0.5 1.8 -3.2 0.4]    --&gt; 2nd entry of utterance 'iccc'
 [-2.2 5.0 1.8 3.7]    --&gt; 3rd entry of utterance 'iccc'
 [-3.8 0.1 1.4 3.3]    --&gt; 4th entry of utterance 'iccc'
 [0.1 2.7 3.5 -1.7]]   --&gt; 5th entry of utterance 'iccc'
</code></pre>
<dl>
<dt>Here <code>N=5</code>, <code>D=4</code>.</dt>
<dt><strong><code>args</code></strong></dt>
<dd>Inference configurations. See <code>arguments.py</code> for details.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>predicted_cluster_id</code></strong></dt>
<dd>predicted speaker id sequence, which is
an array of integers, of size <code>N</code>.
For example, <code>predicted_cluster_id = [0, 1, 0, 0, 1]</code></dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>TypeError</code></strong></dt>
<dd>If test_sequence is of wrong type.</dd>
<dt><strong><code>ValueError</code></strong></dt>
<dd>If test_sequence has wrong dimension.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def predict_single(self, test_sequence, args):
  &#34;&#34;&#34;Predict labels for a single test sequence using UISRNN model.

  Args:
    test_sequence: the test observation sequence, which is 2-dim numpy array
      of real numbers, of size `N * D`.

      - `N`: length of one test utterance.
      - `D` : observation dimension.

      For example:
    ```
    test_sequence =
    [[2.2 -1.0 3.0 5.6]    --&gt; 1st entry of utterance &#39;iccc&#39;
     [0.5 1.8 -3.2 0.4]    --&gt; 2nd entry of utterance &#39;iccc&#39;
     [-2.2 5.0 1.8 3.7]    --&gt; 3rd entry of utterance &#39;iccc&#39;
     [-3.8 0.1 1.4 3.3]    --&gt; 4th entry of utterance &#39;iccc&#39;
     [0.1 2.7 3.5 -1.7]]   --&gt; 5th entry of utterance &#39;iccc&#39;
    ```
      Here `N=5`, `D=4`.
    args: Inference configurations. See `arguments.py` for details.

  Returns:
    predicted_cluster_id: predicted speaker id sequence, which is
      an array of integers, of size `N`.
      For example, `predicted_cluster_id = [0, 1, 0, 0, 1]`

  Raises:
    TypeError: If test_sequence is of wrong type.
    ValueError: If test_sequence has wrong dimension.
  &#34;&#34;&#34;
  # check type
  if (not isinstance(test_sequence, np.ndarray) or
      test_sequence.dtype != float):
    raise TypeError(&#39;test_sequence should be a numpy array of float type.&#39;)
  # check dimension
  if test_sequence.ndim != 2:
    raise ValueError(&#39;test_sequence must be 2-dim array.&#39;)
  # check size
  test_sequence_length, observation_dim = test_sequence.shape
  if observation_dim != self.observation_dim:
    raise ValueError(&#39;test_sequence does not match the dimension specified &#39;
                     &#39;by args.observation_dim.&#39;)

  self.rnn_model.eval()
  test_sequence = np.tile(test_sequence, (args.test_iteration, 1))
  test_sequence = autograd.Variable(
      torch.from_numpy(test_sequence).float()).to(self.device)
  # bookkeeping for beam search
  beam_set = [BeamState()]
  for num_iter in np.arange(0, args.test_iteration * test_sequence_length,
                            args.look_ahead):
    max_clusters = max([len(beam_state.mean_set) for beam_state in beam_set])
    look_ahead_seq = test_sequence[num_iter:  num_iter + args.look_ahead, :]
    look_ahead_seq_length = look_ahead_seq.shape[0]
    score_set = float(&#39;inf&#39;) * np.ones(
        np.append(
            args.beam_size, max_clusters + 1 + np.arange(
                look_ahead_seq_length)))
    for beam_rank, beam_state in enumerate(beam_set):
      beam_score_set = self._calculate_score(beam_state, look_ahead_seq)
      score_set[beam_rank, :] = np.pad(
          beam_score_set,
          np.tile([[0, max_clusters - len(beam_state.mean_set)]],
                  (look_ahead_seq_length, 1)), &#39;constant&#39;,
          constant_values=float(&#39;inf&#39;))
    # find top scores
    score_ranked = np.sort(score_set, axis=None)
    score_ranked[score_ranked == float(&#39;inf&#39;)] = 0
    score_ranked = np.trim_zeros(score_ranked)
    idx_ranked = np.argsort(score_set, axis=None)
    updated_beam_set = []
    for new_beam_rank in range(
        np.min((len(score_ranked), args.beam_size))):
      total_idx = np.unravel_index(idx_ranked[new_beam_rank],
                                   score_set.shape)
      prev_beam_rank = total_idx[0]
      cluster_seq = total_idx[1:]
      updated_beam_state = self._update_beam_state(
          beam_set[prev_beam_rank], look_ahead_seq, cluster_seq)
      updated_beam_set.append(updated_beam_state)
    beam_set = updated_beam_set
  predicted_cluster_id = beam_set[0].trace[-test_sequence_length:]
  return predicted_cluster_id</code></pre>
</details>
</dd>
<dt id="uisrnn.uisrnn.UISRNN.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filepath)</span>
</code></dt>
<dd>
<section class="desc"><p>Save the model to a file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>the path of the file.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save(self, filepath):
  &#34;&#34;&#34;Save the model to a file.

  Args:
    filepath: the path of the file.
  &#34;&#34;&#34;
  torch.save({
      &#39;rnn_state_dict&#39;: self.rnn_model.state_dict(),
      &#39;rnn_init_hidden&#39;: self.rnn_init_hidden.detach().cpu().numpy(),
      &#39;transition_bias&#39;: self.transition_bias,
      &#39;transition_bias_denominator&#39;: self.transition_bias_denominator,
      &#39;crp_alpha&#39;: self.crp_alpha,
      &#39;sigma2&#39;: self.sigma2.detach().cpu().numpy()}, filepath)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="uisrnn" href="index.html">uisrnn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="uisrnn.uisrnn.parallel_predict" href="#uisrnn.uisrnn.parallel_predict">parallel_predict</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="uisrnn.uisrnn.BeamState" href="#uisrnn.uisrnn.BeamState">BeamState</a></code></h4>
<ul class="">
<li><code><a title="uisrnn.uisrnn.BeamState.__init__" href="#uisrnn.uisrnn.BeamState.__init__">__init__</a></code></li>
<li><code><a title="uisrnn.uisrnn.BeamState.append" href="#uisrnn.uisrnn.BeamState.append">append</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="uisrnn.uisrnn.CoreRNN" href="#uisrnn.uisrnn.CoreRNN">CoreRNN</a></code></h4>
<ul class="">
<li><code><a title="uisrnn.uisrnn.CoreRNN.__init__" href="#uisrnn.uisrnn.CoreRNN.__init__">__init__</a></code></li>
<li><code><a title="uisrnn.uisrnn.CoreRNN.forward" href="#uisrnn.uisrnn.CoreRNN.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="uisrnn.uisrnn.UISRNN" href="#uisrnn.uisrnn.UISRNN">UISRNN</a></code></h4>
<ul class="two-column">
<li><code><a title="uisrnn.uisrnn.UISRNN.__init__" href="#uisrnn.uisrnn.UISRNN.__init__">__init__</a></code></li>
<li><code><a title="uisrnn.uisrnn.UISRNN.fit" href="#uisrnn.uisrnn.UISRNN.fit">fit</a></code></li>
<li><code><a title="uisrnn.uisrnn.UISRNN.fit_concatenated" href="#uisrnn.uisrnn.UISRNN.fit_concatenated">fit_concatenated</a></code></li>
<li><code><a title="uisrnn.uisrnn.UISRNN.load" href="#uisrnn.uisrnn.UISRNN.load">load</a></code></li>
<li><code><a title="uisrnn.uisrnn.UISRNN.predict" href="#uisrnn.uisrnn.UISRNN.predict">predict</a></code></li>
<li><code><a title="uisrnn.uisrnn.UISRNN.predict_single" href="#uisrnn.uisrnn.UISRNN.predict_single">predict_single</a></code></li>
<li><code><a title="uisrnn.uisrnn.UISRNN.save" href="#uisrnn.uisrnn.UISRNN.save">save</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>